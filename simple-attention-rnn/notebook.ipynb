{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "INDEX_FROM = 3\n",
    "SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "NUM_CLASSES = 1\n",
    "MODEL_PATH = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trn: [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])] \n",
      "\n",
      " number of counts: (25000,)\n",
      "\n",
      " y_trn: [1 0] \n",
      "\n",
      " number of counts: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_trn:', X_train[:2,], '\\n\\n number of counts:', X_train.shape)\n",
    "print('\\n y_trn:', y_train[:2,], '\\n\\n number of counts:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequences pre-processing: word ids들중에 가장 높은 ids를 찾아 vocab size를 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary_size(X):\n",
    "    return max([max(x) for x in X]) + 1  # plus the 0th word\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_test가 사전에 정의해 논 vocab ids안에 포함되는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_in_vocabulary(X, voc_size):\n",
    "    return [[w for w in x if w < voc_size] for x in X]\n",
    "\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "\n",
    "assert X_test == X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전에 정의해논 seq_length에 맞도록 zero_padding을 설정해 준다 on training and testset\n",
    "     - [0]* (m_seq - len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, seq_len):\n",
    "    return np.array([x[:seq_len ] + [0] * max(seq_len - len(x), 0) for x in X])\n",
    "\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 0, 0],\n",
       "       [3, 4, 2, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_pad([[1,2,3],[3,4,2]],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.variable_scope('Inputs'):\n",
    "    batch_size_ph = tf.placeholder(tf.int32, [], name='batch_size_ph')\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')\n",
    "    lr_ph = tf.placeholder(tf.float32, name='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Inputs/batch_size_ph:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donghwa/anaconda3/envs/lab/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.variable_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.get_variable(\n",
    "            \"w_embed\",\n",
    "            shape= [vocabulary_size, EMBEDDING_DIM], \n",
    "            initializer=tf.initializers.truncated_normal(stddev=0.1)\n",
    "            )\n",
    "    \n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    \n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Embedding_layer/w_embed:0' shape=(10000, 100) dtype=float32_ref>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Embedding_layer/embedding_lookup/Identity:0' shape=(?, 250, 100) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cardinal integer (n, 250) -> (n, 250, 100)\n",
    "batch_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-ab7008a722df>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "Tensor(\"fw/GRUCellZeroState/zeros:0\", shape=(?, 150), dtype=float32)\n",
      "Tensor(\"bw/GRUCellZeroState/zeros:0\", shape=(?, 150), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('fw'):\n",
    "    fw_cell = tf.nn.rnn_cell.GRUCell(num_units=HIDDEN_SIZE)\n",
    "    fw_init_state = fw_cell.zero_state(batch_size_ph, tf.float32)\n",
    "with tf.variable_scope('bw'):\n",
    "    bw_cell = tf.nn.rnn_cell.GRUCell(num_units=HIDDEN_SIZE)\n",
    "    bw_init_state = bw_cell.zero_state(batch_size_ph, tf.float32)\n",
    "\n",
    "\n",
    "print(fw_init_state)\n",
    "print(bw_init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-c3556026aaed>:5: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/donghwa/anaconda3/envs/lab/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "states_series, current_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                               cell_bw=bw_cell,\n",
    "                                                               inputs= batch_embedded,\n",
    "                                                               initial_state_fw = fw_init_state,\n",
    "                                                               initial_state_bw = bw_init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'bidirectional_rnn/fw/fw/transpose_1:0' shape=(256, 250, 150) dtype=float32>,\n",
       " <tf.Tensor 'ReverseV2:0' shape=(256, 250, 150) dtype=float32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(256, 150) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(256, 150) dtype=float32>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.histogram('RNN_outputs', states_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_major = False\n",
    "attention_size = ATTENTION_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(states_series, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(256, 250, 300) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat in bi-direction\n",
    "inputs = tf.concat(states_series, 2)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in general, time_major = False if (B,T,D) \n",
    "if time_major:\n",
    "    # (T,B,D) => (B,T,D)\n",
    "    inputs = tf.transpose(inputs, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer with concatenation if bi-RNN\n",
    "hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "w_omega = tf.get_variable(\"w_omega\", \n",
    "                          shape = [hidden_size, attention_size],\n",
    "                          initializer = tf.initializers.truncated_normal(stddev=0.1))\n",
    "b_omega = tf.get_variable(\"b_omega\", \n",
    "                          shape = [attention_size],\n",
    "                          initializer = tf.initializers.truncated_normal(stddev=0.1))\n",
    "u_omega = tf.get_variable(\"u_omega\", \n",
    "                          shape = [attention_size],\n",
    "                          initializer = tf.initializers.truncated_normal(stddev=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'v/Tanh:0' shape=(256, 250, 50) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope('v'):\n",
    "    # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "    #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "    v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word-level context vector(u_omega)를 사용하여, attetion score를 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'u_omega:0' shape=(50,) dtype=float32_ref>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'vu:0' shape=(256, 250) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "vu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'alphas:0' shape=(256, 250) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(256, 250, 1) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(alphas, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(256, 250, 300) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum:0' shape=(256, 300) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "    '''\n",
    "    input: (B,S,D)\n",
    "    output: attentive (B,D)\n",
    "    '''\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.get_variable(\"w_omega\", \n",
    "                              shape = [hidden_size, attention_size],\n",
    "                              initializer = tf.initializers.truncated_normal(stddev=0.1))\n",
    "    b_omega = tf.get_variable(\"b_omega\", \n",
    "                              shape = [attention_size],\n",
    "                              initializer = tf.initializers.zeros())\n",
    "    u_omega = tf.get_variable(\"u_omega\", \n",
    "                              shape = [attention_size],\n",
    "                              initializer = tf.initializers.truncated_normal(stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout/mul:0' shape=(256, 300) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = tf.nn.dropout(attention_output, rate = 1-keep_prob_ph)\n",
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Fully_connected_layer/xw_plus_b:0' shape=(256, 1) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully connected layer\n",
    "with tf.variable_scope('Fully_connected_layer'):\n",
    "    W = tf.get_variable(\"W\", \n",
    "                        shape = [HIDDEN_SIZE * 2, NUM_CLASSES], # Hidden size is multiplied by 2 for Bi-RNN\n",
    "                        initializer = tf.initializers.truncated_normal(stddev=0.1))\n",
    "    b = tf.get_variable(\"b\", \n",
    "                        shape = [NUM_CLASSES], \n",
    "                        initializer = tf.initializers.zeros())\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    \n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze:0' shape=(256,) dtype=float32>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = tf.squeeze(y_hat)\n",
    "tf.summary.histogram('W', W)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donghwa/anaconda3/envs/lab/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_ph).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   14,   22, ...,    0,    0,    0],\n",
       "       [   1,  194, 1153, ...,    0,    0,    0],\n",
       "       [   1,   14,   47, ...,    0,    0,    0],\n",
       "       [   1,    4,    2, ...,   94,  318, 1382],\n",
       "       [   1,  249, 1323, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_generator(X, y, batch_size):\n",
    "    \"\"\"Primitive batch generator \n",
    "    \"\"\"\n",
    "    size = X.shape[0]\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    X_copy = X_copy[indices]\n",
    "    y_copy = y_copy[indices]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + batch_size <= size:\n",
    "            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n",
    "            i += batch_size\n",
    "        else:\n",
    "            i = 0\n",
    "            indices = np.arange(size)\n",
    "            np.random.shuffle(indices)\n",
    "            X_copy = X_copy[indices]\n",
    "            y_copy = y_copy[indices]\n",
    "            continue\n",
    "\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   48,   13, ...,    0,    0,    0],\n",
       "       [   1,   14,    9, ...,    0,    0,    0],\n",
       "       [   1,   14,   22, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    2,  852, ...,    0,    0,    0],\n",
       "       [   1,   14,    9, ...,    0,    0,    0],\n",
       "       [   1,  449, 3214, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   48,   13, ...,    0,    0,    0],\n",
       "       [   1,   14,    9, ...,    0,    0,    0],\n",
       "       [   1,   14,   22, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    2,  852, ...,    0,    0,    0],\n",
       "       [   1,   14,    9, ...,    0,    0,    0],\n",
       "       [   1,  449, 3214, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([139, 126,  93, 114, 250, 250, 250, 250, 250,  49, 162, 115, 192,\n",
       "        89,  95, 206, 250, 250, 135, 132,  90, 216, 250, 186, 250, 243,\n",
       "       136, 250, 116, 120, 250, 250, 123, 159, 250,  78, 250, 166, 137,\n",
       "       180, 250,  89, 104,  86, 201, 181, 250, 250, 113, 127, 128, 108,\n",
       "       250,  56, 227, 250, 170, 139, 250, 114, 154, 133, 250, 127, 138,\n",
       "       250, 242, 250,  73,  80, 104, 250, 250, 250, 250, 190, 230, 213,\n",
       "       146, 152, 207, 250,  76, 250, 129,  47, 176, 187, 103, 170, 126,\n",
       "       201, 250, 121, 117, 150, 157, 250, 125, 190, 250, 250, 250, 153,\n",
       "        94, 155, 250,  51, 250,  63, 250, 191, 250, 167, 250, 250, 245,\n",
       "       250, 215, 250, 154, 250, 250,  51, 250, 250, 201, 135, 138, 133,\n",
       "       250, 129,  92, 250, 152, 250, 163, 119, 250, 130, 223,  44, 107,\n",
       "       140, 250,  36, 219, 195, 250, 119, 138, 250, 132, 223, 250, 139,\n",
       "       123, 163, 119, 127, 189, 250, 209, 250, 197, 124, 250,  47, 158,\n",
       "       126, 139, 250, 186, 120, 250, 201, 210, 112, 250, 125, 250, 150,\n",
       "       150, 250, 145, 188, 250, 162, 250, 216, 114, 156, 250, 145, 198,\n",
       "       250, 250, 187, 134, 153, 120, 250,  38, 250, 131,  50, 130, 218,\n",
       "       250,  50, 178, 162, 250, 121, 173, 250, 109, 171, 218, 123,  88,\n",
       "       156, 250, 250,  27, 250, 250, 115, 244, 245, 174, 151, 250,  69,\n",
       "       174, 206, 250, 250, 250, 223, 173, 250,  58, 250, 129, 184, 142,\n",
       "       250, 130, 100, 124, 250, 136, 109, 156, 160])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x_batch !=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [01:12<00:00,  1.36it/s]\n",
      "100%|██████████| 97/97 [01:10<00:00,  1.38it/s]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.304, val_loss: 0.336, acc: 0.737, val_acc: 0.856\n",
      "epoch: 1\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [01:10<00:00,  1.35it/s]\n",
      "100%|██████████| 97/97 [01:10<00:00,  1.37it/s]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.227, val_loss: 0.309, acc: 0.900, val_acc: 0.868\n",
      "epoch: 2\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [01:10<00:00,  1.33it/s]\n",
      "100%|██████████| 97/97 [01:10<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.203, val_loss: 0.326, acc: 0.928, val_acc: 0.863\n",
      "Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.sum(x_batch !=0, axis=1)  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB,\n",
    "                                                               lr_ph: 0.001,\n",
    "                                                               batch_size_ph:BATCH_SIZE})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.sum(x_batch !=0, axis=1)  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0,\n",
    "                                                                    lr_ph: 0.001,\n",
    "                                                                    batch_size: BATCH_SIZE})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! tensorboard --logdir=./logdir --host "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
